{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "cell_markers": "\"\"\""
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vlMQHViqxP"
      },
      "source": [
        "# Requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwiEKwWBiqxa"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "! git clone https://github.com/gaudel/recommender_system.git\n",
        "! mkdir /content/recommender_system/bandits/data\n",
        "import os\n",
        "os.chdir(\"./recommender_system/bandits\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUt9BD_Qiqxb"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1D2QFYDiqxb"
      },
      "source": [
        "# An Arm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kitGiwJ3iqxc"
      },
      "source": [
        "# Instantiate an arm\n",
        "from arm import MyBeta\n",
        "arm = MyBeta(mean=0.2)\n",
        "\n",
        "# draw three values from this arm\n",
        "print(arm.draw())\n",
        "print(arm.draw())\n",
        "print(arm.draw())\n",
        "\n",
        "# expected value\n",
        "print(arm.mean())\n",
        "\n",
        "# empirical mean\n",
        "n_samples = 1000\n",
        "rewards = np.zeros(n_samples)\n",
        "for i in range(n_samples):\n",
        "    rewards[i] = arm.draw()\n",
        "\n",
        "# XXX TO DO XXX       print empirical mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmrokbgYiqxd"
      },
      "source": [
        "# Bandit Setting : One Game (Played by a Human)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKqBAo9iqxf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4XzfVsiiqxf"
      },
      "source": [
        "environment = [MyBeta(mean=0.8), MyBeta(mean=0.2), MyBeta(mean=0.4)]\n",
        "\n",
        "# mean per arm\n",
        "for i, arm in enumerate(environment):\n",
        "    print(\"expected value for arm \", i, \": \", arm.mean(), sep=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip65_8xviqxg"
      },
      "source": [
        "## Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIHgz6gkiqxh"
      },
      "source": [
        "n_iter = 5\n",
        "\n",
        "# play a game\n",
        "for t in range(n_iter):\n",
        "    print(\"===\")\n",
        "    print(\"iteration \", t)\n",
        "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
        "    reward = 0 # XXX TO DO XXX       draw the chosen arm and store the result in reward\n",
        "    print(\"Reward:\", reward)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptxGDSGJiqxi"
      },
      "source": [
        "## Cumulative Gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5RN0M3giqxj"
      },
      "source": [
        "n_iter = 5\n",
        "\n",
        "rewards = np.zeros(n_iter)\n",
        "\n",
        "# play a game\n",
        "for t in range(n_iter):\n",
        "    print(\"===\")\n",
        "    print(\"iteration \", t)\n",
        "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
        "    reward = environment[i_arm].draw()\n",
        "    # XXX TO DO XXX       store the reward in `rewards`\n",
        "    print(\"Reward:\", reward)\n",
        "\n",
        "    \n",
        "# XXX TO DO XXX       print total reward at time `n_iter`\n",
        "\n",
        "    \n",
        "# XXX TO DO XXX       print total reward at each timestep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpzQsLa_iqxj"
      },
      "source": [
        "# Cumulative Regret"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spsAWrrHiqxk"
      },
      "source": [
        "n_iter = 5\n",
        "\n",
        "expected_rewards = np.zeros(n_iter)\n",
        "\n",
        "best_expected_rewards = np.zeros(n_iter)\n",
        "\n",
        "\n",
        "# play a game\n",
        "for t in range(n_iter):\n",
        "    print(\"===\")\n",
        "    print(\"iteration \", t)\n",
        "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
        "    reward = environment[i_arm].draw()\n",
        "    print(\"Reward:\", reward)\n",
        "    # to compute the regret\n",
        "    expected_rewards[t] = environment[i_arm].mean()\n",
        "    best_expected_rewards[t] = max([arm.mean() for arm in environment])\n",
        "\n",
        "    \n",
        "# XXX TO DO XXX       print instantaneous regret\n",
        "\n",
        "    \n",
        "# XXX TO DO XXX       print cumulative regret    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvhhEPhliqxk"
      },
      "source": [
        "## Let's Plot!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ8EifT7iqxl"
      },
      "source": [
        "plt.plot((best_expected_rewards - expected_rewards).cumsum(), \"--\", label='Human Intelligence')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Cumulative Regret')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzGg8jMsiqxl"
      },
      "source": [
        "# AI Player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kECORi-5iqxl"
      },
      "source": [
        "## Choose an Arm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6HKYB8xiqxl"
      },
      "source": [
        "from player import EpsilonNGreedy\n",
        "\n",
        "player = EpsilonNGreedy(nb_arms=3, c=5)\n",
        "\n",
        "for _ in range(30):\n",
        "    print(\"Chosen arm:\", player.choose_next_arm())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlL8ztdoiqxl"
      },
      "source": [
        "## Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj-ch211iqxm"
      },
      "source": [
        "n_rep = 10\n",
        "\n",
        "# play arm 2 `n_rep` times\n",
        "i_arm = 2\n",
        "for _ in range(n_rep):\n",
        "    reward = environment[i_arm].draw()\n",
        "    player.update(i_arm, reward)\n",
        "\n",
        "\n",
        "# XXX TO DO XXX       play arm 0 and arm 1 `n_rep` times\n",
        "\n",
        "\n",
        "\n",
        "# What are the chosen arms now ?\n",
        "for _ in range(30):\n",
        "    print(\"Chosen arm:\", player.choose_next_arm())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtVCmBtiqxn"
      },
      "source": [
        "# Let's Play!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh5Cc7x8iqxn"
      },
      "source": [
        "n_iter = 1000\n",
        "\n",
        "\"\"\"!!! Do not cheat: restart the player !!!\"\"\"\n",
        "player.restart()\n",
        "\n",
        "\n",
        "expected_rewards = np.zeros(n_iter)\n",
        "best_expected_rewards = np.zeros(n_iter)\n",
        "\n",
        "\n",
        "# play a game\n",
        "for t in range(n_iter):\n",
        "    i_arm = 0 # XXX TO DO XXX       let the artificial player choose the arm\n",
        "    reward = environment[i_arm].draw()\n",
        "    # XXX TO DO XXX       tel to the artificial player which regret was obtained\n",
        "    # to compute the regret\n",
        "    expected_rewards[t] = environment[i_arm].mean()\n",
        "    best_expected_rewards[t] = max([arm.mean() for arm in environment])\n",
        "\n",
        "\n",
        "# plot the results    \n",
        "plt.plot((best_expected_rewards - expected_rewards).cumsum(), \"--\", label='EG c=5')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Cumulative Regret')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBSggruiqxo"
      },
      "source": [
        "# Multiple Games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQKxFmgaiqxo"
      },
      "source": [
        "n_iter = 1000\n",
        "n_games = 5\n",
        "\n",
        "expected_rewards = np.zeros((n_iter, n_games))\n",
        "best_expected_rewards = np.zeros((n_iter, n_games))\n",
        "\n",
        "\n",
        "\n",
        "for i_game in range(n_games):\n",
        "    \"\"\"!!! Do not cheat: restart the player !!!\"\"\"\n",
        "    player.restart()\n",
        "\n",
        "    # play a game\n",
        "    for t in range(n_iter):\n",
        "        i_arm = player.choose_next_arm()\n",
        "        reward = environment[i_arm].draw()\n",
        "        player.update(i_arm, reward)\n",
        "        # to compute the regret\n",
        "        expected_rewards[t, i_game] = environment[i_arm].mean()\n",
        "        best_expected_rewards[t, i_game] = max([arm.mean() for arm in environment])\n",
        "\n",
        "\n",
        "# plot the results    \n",
        "for i_game in range(n_games):\n",
        "    plt.plot((best_expected_rewards - expected_rewards)[:,i_game].cumsum(), \"--\", label='game '+str(i_game))\n",
        "plt.plot((best_expected_rewards - expected_rewards).mean(1).cumsum(), \"-\", label='average', lw=3, color = 'black')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Cumulative Regret')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZk-4ixoiqxo"
      },
      "source": [
        "# Let `play_games.py` do the Job "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAalfGFoLHp"
      },
      "source": [
        "# run\n",
        "%run -t play_games.py 200 10 --Random --Ber 0.4 0.2 0.8\n",
        "%run -t play_games.py 200 10 --Oracle --Ber 0.4 0.2 0.8\n",
        "%run -t play_games.py 200 10 --EtC 20 --Ber 0.4 0.2 0.8\n",
        "%run -t play_games.py 200 10 --eGreedy 1 --Ber 0.4 0.2 0.8\n",
        "# XXX TO DO XXX       run also with c=10 and c=100\n",
        "%run -t play_games.py 200 10 --TS --Ber 0.4 0.2 0.8\n",
        "\n",
        "# load\n",
        "from tools import retrieve_data_from_zip\n",
        "logs = []\n",
        "logs.extend(retrieve_data_from_zip(\"data/Ber0.4_0.2_0.8__Random__nb_trials_200__nb_games_10.gz\"))\n",
        "logs.extend(retrieve_data_from_zip(\"data/Ber0.4_0.2_0.8__Oracle__nb_trials_200__nb_games_10.gz\"))\n",
        "logs.extend(retrieve_data_from_zip(\"data/Ber0.4_0.2_0.8__EtC__m_20__nb_trials_200__nb_games_10.gz\"))\n",
        "logs.extend(retrieve_data_from_zip(\"data/Ber0.4_0.2_0.8__eGreedy__c_1__nb_trials_200__nb_games_10.gz\"))\n",
        "# XXX TO DO XXX       plot also with c=10 and c=100\n",
        "logs.extend(retrieve_data_from_zip(\"data/Ber0.4_0.2_0.8__TS__nb_trials_200__nb_games_10.gz\"))\n",
        "\n",
        "# plot\n",
        "from exp import plot_exp\n",
        "plot_exp(logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbNX3hsBiqxp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}