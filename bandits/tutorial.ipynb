{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an arm\n",
    "from arm import MyBeta\n",
    "arm = MyBeta(mean=0.2)\n",
    "\n",
    "# draw three values from that arm\n",
    "print(arm.draw())\n",
    "print(arm.draw())\n",
    "print(arm.draw())\n",
    "\n",
    "# expected value\n",
    "print(arm.mean())\n",
    "\n",
    "# empirical mean\n",
    "n_samples = 1000\n",
    "rewards = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    rewards[i] = arm.draw()\n",
    "\n",
    "# XXX TO DO XXX       print empirical mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Setting : One Game (Played by a Human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = [MyBeta(mean=0.8), MyBeta(mean=0.2), MyBeta(mean=0.4)]\n",
    "\n",
    "# mean per arm\n",
    "for i, arm in enumerate(environment):\n",
    "    print(\"expected value for arm \", i, \": \", arm.mean(), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "\n",
    "# play a game\n",
    "for t in range(n_iter):\n",
    "    print(\"===\")\n",
    "    print(\"iteration \", t)\n",
    "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
    "    reward = 0 # XXX TO DO XXX       draw the chosen arm and store the result in reward\n",
    "    print(\"Reward:\", reward)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "\n",
    "rewards = np.zeros(n_iter)\n",
    "\n",
    "# play a game\n",
    "for t in range(n_iter):\n",
    "    print(\"===\")\n",
    "    print(\"iteration \", t)\n",
    "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
    "    reward = environment[i_arm].draw()\n",
    "    # XXX TO DO XXX       store the reward in `rewards`\n",
    "    print(\"Reward:\", reward)\n",
    "\n",
    "    \n",
    "# XXX TO DO XXX       print total reward at time `n_iter`\n",
    "\n",
    "    \n",
    "# XXX TO DO XXX       print total reward at each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "\n",
    "expected_rewards = np.zeros(n_iter)\n",
    "\n",
    "best_expected_rewards = np.zeros(n_iter)\n",
    "\n",
    "\n",
    "# play a game\n",
    "for t in range(n_iter):\n",
    "    print(\"===\")\n",
    "    print(\"iteration \", t)\n",
    "    i_arm = int(input(\"Which arm do you want to play? \"))\n",
    "    reward = environment[i_arm].draw()\n",
    "    print(\"Reward:\", reward)\n",
    "    # to compute the regret\n",
    "    expected_rewards[t] = environment[i_arm].mean()\n",
    "    best_expected_rewards[t] = max([arm.mean() for arm in environment])\n",
    "\n",
    "    \n",
    "# XXX TO DO XXX       print instantaneous regret\n",
    "\n",
    "    \n",
    "# XXX TO DO XXX       print cumulative regret    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((best_expected_rewards - expected_rewards).cumsum(), \"--\", label='Human Intelligence')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an Arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from player import EpsilonNGreedy\n",
    "\n",
    "player = EpsilonNGreedy(nb_arms=3, c=5)\n",
    "\n",
    "for _ in range(30):\n",
    "    print(\"Chosen arm:\", player.choose_next_arm())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 10\n",
    "\n",
    "# play arm 2 `n_rep` times\n",
    "i_arm = 2\n",
    "for _ in range(n_rep):\n",
    "    reward = environment[i_arm].draw()\n",
    "    player.update(i_arm, reward)\n",
    "\n",
    "\n",
    "# XXX TO DO XXX       play arm 0 and arm 1 `n_rep` times\n",
    "\n",
    "\n",
    "\n",
    "# What are the chosen arms now ?\n",
    "for _ in range(30):\n",
    "    print(\"Chosen arm:\", player.choose_next_arm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "\"\"\"!!! Do not cheat: restart the player !!!\"\"\"\n",
    "player.restart()\n",
    "\n",
    "\n",
    "expected_rewards = np.zeros(n_iter)\n",
    "best_expected_rewards = np.zeros(n_iter)\n",
    "\n",
    "\n",
    "# play a game\n",
    "for t in range(n_iter):\n",
    "    i_arm = 0 # XXX TO DO XXX       let the artificial player choose the arm\n",
    "    reward = environment[i_arm].draw()\n",
    "    # XXX TO DO XXX       tel to the artificial player which regret was obtained\n",
    "    # to compute the regret\n",
    "    expected_rewards[t] = environment[i_arm].mean()\n",
    "    best_expected_rewards[t] = max([arm.mean() for arm in environment])\n",
    "\n",
    "\n",
    "# plot the results    \n",
    "plt.plot((best_expected_rewards - expected_rewards).cumsum(), \"--\", label='EG c=5')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "n_games = 5\n",
    "\n",
    "expected_rewards = np.zeros((n_iter, n_games))\n",
    "best_expected_rewards = np.zeros((n_iter, n_games))\n",
    "\n",
    "\n",
    "\n",
    "for i_game in range(n_games):\n",
    "    \"\"\"!!! Do not cheat: restart the player !!!\"\"\"\n",
    "    player.restart()\n",
    "\n",
    "    # play a game\n",
    "    for t in range(n_iter):\n",
    "        i_arm = player.choose_next_arm()\n",
    "        reward = environment[i_arm].draw()\n",
    "        player.update(i_arm, reward)\n",
    "        # to compute the regret\n",
    "        expected_rewards[t, i_game] = environment[i_arm].mean()\n",
    "        best_expected_rewards[t, i_game] = max([arm.mean() for arm in environment])\n",
    "\n",
    "\n",
    "# plot the results    \n",
    "for i_game in range(n_games):\n",
    "    plt.plot((best_expected_rewards - expected_rewards)[:,i_game].cumsum(), \"--\", label='game '+str(i_game))\n",
    "plt.plot((best_expected_rewards - expected_rewards).mean(1).cumsum(), \"-\", label='average', lw=3, color = 'black')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let `exp.py` do the Job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp import games, cumulative_regret, cumulative_reward, logarithmic_indices\n",
    "from random import shuffle\n",
    "from player import Oracle, EpsilonNGreedy, ThompsonSamplingBernoulli\n",
    "\n",
    "# =============================\n",
    "# play games\n",
    "# =============================\n",
    "environment = [MyBeta(mean=0.8), MyBeta(mean=0.2), MyBeta(mean=0.4)]\n",
    "shuffle(environment)\n",
    "n_iter = 1000\n",
    "n_games = 5\n",
    "\n",
    "logs_oracle = games(Oracle(np.argmax([a.mean() for a in environment])), environment, n_iter, n_games)\n",
    "logs_EG1 = games(EpsilonNGreedy(nb_arms=len(environment), c=1), environment, n_iter, n_games)\n",
    "# XXX TO DO XXX       run also with c=10 and c=100\n",
    "logs_TS = games(ThompsonSamplingBernoulli(nb_arms=len(environment), prior_s=0.5, prior_f=0.5), environment, n_iter, n_games)\n",
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "# plot cumulative regret\n",
    "# =============================\n",
    "plt.clf()\n",
    "\n",
    "inds = logarithmic_indices(n_iter, 100) # do not plot each point (too much with long runs)\n",
    "\n",
    "plt.plot(inds + 1, cumulative_regret(logs_oracle)[inds], label='Oracle')\n",
    "plt.plot(inds + 1, cumulative_regret(logs_EG1)[inds], \"--\", label='EG, c = 1')\n",
    "# XXX TO DO XXX       plot res for c=10 and c=100\n",
    "plt.plot(inds + 1, cumulative_regret(logs_TS)[inds], \"--\", label='TS')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Expected Cumulative Regret')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.loglog()\n",
    "plt.show()\n",
    "\n",
    "# =============================\n",
    "# plot cumulative reward\n",
    "# =============================\n",
    "plt.clf()\n",
    "\n",
    "inds = logarithmic_indices(n_iter, 100) # do not plot each point (too much with long runs)\n",
    "\n",
    "plt.plot(inds + 1, cumulative_reward(logs_oracle)[inds], label='Oracle')\n",
    "plt.plot(inds + 1, cumulative_reward(logs_EG1)[inds], \"--\", label='EG, c = 1')\n",
    "# XXX TO DO XXX       plot res for c=10 and c=100\n",
    "plt.plot(inds + 1, cumulative_reward(logs_TS)[inds], \"--\", label='TS')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Expected Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.loglog()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
